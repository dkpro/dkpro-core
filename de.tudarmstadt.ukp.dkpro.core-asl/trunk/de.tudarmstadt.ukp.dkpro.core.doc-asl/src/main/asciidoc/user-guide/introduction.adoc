// Copyright 2013
// Ubiquitous Knowledge Processing (UKP) Lab
// Technische Universität Darmstadt
// 
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
// 
// http://www.apache.org/licenses/LICENSE-2.0
// 
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

[[sect_introduction]]

== Introduction

Many powerful and state-of-the-art NLP tools are already freely available in the NLP
research community. New and improved tools are being developed and released continuously.
The tools cover the whole range of NLP-related processing tasks. DKPro Core provides UIMA
components wrapping these tools so they can be used interchangeably in processing pipelines.
DKPro Core builds heavily on uimaFIT which allows for rapid and easy development of
UIMA-based NLP processing pipelines.

=== Choosing components

Sometimes we get asked which parser, tagger, etc. is the best and which should be
used. We currently do not make any evaluations of the integrated tools. Also, building a
pipeline just of the "best" components may not actually yield the best results, because
of several reasons:

* components or models may expect different tokenizations or tagsets
* components or models may be good for one domain (e.g. news) but not for
  another (e.g. twitter data)

We recommend that you try various combinations and stick with the one that gives the
best result for __your__ data.

==== Compatibility of Components

When selecting components for your pipeline you should make sure that the
components are compatible regarding the annotation types they expect or
offer.


* if a component expects an annotation type that is not provided by the
  preceding component, that may lead to an error or simply to no
  results
* if a component (e.g. a reader which adds sentence annotations) provides an
  annotation that is added again by a subsequent component (e.g. a segmenter),
  that will result in undefined behaviour of other components when you iterate
  over the annotation that has been added more than once.

To check whether components are compatible, you can look at the
++@TypeCapability++ annotation which is available in most DKPro
Core components. Mind that many components can be configured with regards to which
types they consume or produce, so the ++@TypeCapability++ should be
taken as a rough indicator, not as a definitive information. It is also important to
note, that the ++@TypeCapability++ does say anything about the tagset
being consumed or produced by a component. E.g. one if a POS-tagger uses a model
that produces POS-tags from the tagset X and a dependency-parser uses a model that
requires POS-tags from the tagset Y, then the two models are not semantically
compatible - even though the POS-tagger and dependency-parser components are
compatible on the level of the type system.


==== Particular Settings


===== Dictionaries and other lexical resources

If you use components in your pipeline that access dictionaries or other
lexical resources, it might be essential to include a Lemmatizer in your
pipeline: Many dictionaries and well-known lexical resources such as WordNet
require at minimum a lemma form as a search word in order to return information
about that word. For large-scale lexical resources, e.g. for Wiktionary,
additional information about POS is very helpful in order to reduce the
ambiguity of a given lemma form.


===== Lemmatizing multiwords 

If you use lemma information in your pipeline, you should bear in mind that
multiword expressions, in particular discontinuous multiwords, might not be
lemmatized as one word (or expression), but rather each multiword part might be
lemmatized separately. In languages such as German, there are verbs with
separable particle such as _anfangen_ (_an_ occurs separate from _fangen_ in
particular sentence constructions). Therefore - depending on your use case - you
might consider postprocessing the output of the lemmatizer in order to get the
true lemmas (which you might need, e.g. in order to look up information in a
lexical resource).


===== Morphologically Rich Languages


* Parsing: Morphologically rich languages (e.g. Czech, German, and
  Hungarian) pose a particular challenge to parser components (Tsarfaty et
  al. 2013).


* Morphological analysis: for languages with case syncretism (displaying
  forms that are ambiguous regarding their case, e.g. _Frauen_ in German
  can be nominative or genitive or dative or accusative), it might be
  better to leave case underspecified at the morphosyntactic level and
  leave disambiguation to the components at the syntactic level. Otherwise
  errors might be introduced that will then be propagated to the next
  pipeline component (Seeker and Kuhn 2013).


===== Domain-specific and other non-standard data

Most components (sentence splitters, POS taggers, parsers ...) are trained on
(standard) newspaper text. As a consequence, you might encounter a significant
performance drop if you apply the components to domain specific or other
non-standard data without adaptation.

* Tokenizing: adapting the tokenizer to your specific domain is crucial,
  since tokenizer errors propagate to all subsequent components in the
  pipeline and worsen their performance. For example, you might adapt your
  tokenizer to become aware of emoticons or chemical formulae in order to
  process social media data or text from the biochemical domain.


===== Shallow processing and POS tagsets

While more advanced semantic processing (e.g. discourse analysis) typically
depends on the output of a parser component, there might be settings where you
prefer to perform shallow processing (i.e. POS tagging and chunking).

For shallow processing, it might be necessary to become familiar with the
original POS tagsets of the POS taggers rather than relying on the uniform, but
coarse-grained DKPro Core POS tags (because the original fine-grained POS tags
carry more information).

Although many POS taggers in a given language are trained on the same POS
tagset (e.g. the Penn TreeBank Tagset for English, the STTS Tagset for German),
the individual POS Taggers might output variants of this tagset. You should be
aware of the fact that in the DKPro Core version of the tagger, the original POS
tagger output possibly has been mapped to a version that is compatible with the
corresponding original tagset. (Example)


==== Further references

Here are some further references that might be helpful when deciding which tools to use:

* Giesbrecht, Eugenie and Evert, Stefan (2009). Part-of-speech tagging - a
  solved task? An evaluation of POS taggers for the Web as corpus. In I.
  Alegria, I. Leturia, and S. Sharoff, editors, Proceedings of the 5th Web as
  Corpus Workshop (WAC5), San Sebastian, Spain. 
  link:$$http://purl.org/stefan.evert/PUB/GiesbrechtEvert2009_Tagging.pdf$$[PDF]

* Reut Tsarfaty, Djamé Seddah, Sandra Kübler, and Joakim Nivre. 2013.
  Parsing morphologically rich languages: Introduction to the special issue.
  Comput. Linguist. 39, 1 (March 2013), 15-22. link:$$https://aclweb.org/anthology/J/J13/J13-1003.pdf$$[PDF]

* Wolfgang Seeker and Jonas Kuhn. 2013. Morphological and syntactic case in
  statistical dependency parsing. Comput. Linguist. 39, 1 (March 2013), 23-55.
  link:$$http://aclweb.org/anthology//J/J13/J13-1004.pdf$$[PDF]


=== Adding components as dependencies (Maven)

As an example, we take the OpenNlpPosTagger component. To make it available in a
pipeline, we add the following dependency to our POM file:


[source,xml,subs="+attributes"]
----
<properties>
  <dkpro.core.version>{revnumber}</dkpro.core.version>
</properties>
<dependencies>
  <dependency>
    <groupId>de.tudarmstadt.ukp.dkpro.core</groupId>
    <artifactId>de.tudarmstadt.ukp.dkpro.core.opennlp-asl</artifactId>
  </dependency>
</dependencies>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>de.tudarmstadt.ukp.dkpro.core</groupId>
      <artifactId>de.tudarmstadt.ukp.dkpro.core-asl</artifactId>
      <version>${dkpro.core.version}</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
----

The dependency on DKPro Core declared in the dependency management section fixes the
version of all DKPro Core dependencies that are added to the POM. Hence, it is not
necessary to declare the version for each dependency. When upgrading to a new DKPro Core
version, it is sufficient to change the value of the
++dkpro.core.version++ property in the properties section.



=== Adding resources as dependencies (Maven)

Most components require resources such as models in order to operate. Since components
and resources are versioned separately, it can be non-trivial to find the right version
of a resource for a particular version of a component. For this reason, DKPro Core
components each maintain a list of resources known to be compatible with them. This
information can be accessed in a Maven POM, thus avoiding the need to manually specify
the version of the models. Consequently, when you upgrade to a new version of DKPro
Core, all models are automatically upgraded as well. This is usually the desired
solution, although it can mean that your pipelines may produce slightly different
results.

As an example, we take the OpenNlpPosTagger component. In the previous section, we
have seen how to make it available in a pipeline. Now we also add the model for
English.

[source,xml,subs="+attributes"]
----
<dependencies>
  <dependency>
    <groupId>de.tudarmstadt.ukp.dkpro.core</groupId>
    <artifactId>de.tudarmstadt.ukp.dkpro.core.opennlp-model-tagger-en-maxent</artifactId>
  </dependency>
</dependencies>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>de.tudarmstadt.ukp.dkpro.core</groupId>
      <artifactId>de.tudarmstadt.ukp.dkpro.core.opennlp-asl</artifactId>
      <version>${dkpro.core.version}</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
----

The dependency on the DKPro Core OpenNLP module declared in the dependency management
section fixes the version of all known OpenNLP models. Thus, it is not necessary to
declare a version on each model dependency. When upgrading to a new DKPro Core version,
it is sufficient to change the value of the ++dkpro.core.version++
property in the properties section.

