groupId: org.dkpro.core.datasets.conll2000
datasetId: conll2000
# Didn't find any version information at the source, falling back to date of the corpus files
version: 20000221
language: en
mediaType: text/x.org.dkpro.conll-2000

name: CoNLL-2000 NER Shared Task Data (English)
url: http://www.cnts.ua.ac.be/conll2000/chunking/
description: |
  This is the data from the CoNLL-2000 shared task on text chunking.
  The data consists of the same partitions of the Wall Street Journal corpus (WSJ) as the widely 
  used data for noun phrase chunking: sections 15-18 as training data (211727 tokens) and section
  20 as test data (47377 tokens). The annotation of the data has been derived from the WSJ corpus 
  by a program written by Sabine Buchholz from Tilburg University, The Netherlands. Instead of using
  the part-of-speech tags of the WSJ corpus, the data set used tags generated by the Brill tagger.
   
  (This description has been partially copied from the corpus website).
  
roles:
  training:
    - train.txt.gz
  testing: 
    - test.txt.gz
artifacts:
  train.txt.gz:
    url: "http://www.cnts.ua.ac.be/conll2000/chunking/train.txt.gz"
    sha1: 9f31cf936554cebf558d07cce923dca0b7f31864
  test.txt.gz:
    url: "http://www.cnts.ua.ac.be/conll2000/chunking/test.txt.gz"
    sha1: dc57527f1f60eeafad03da51235185141152f849
